{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Damage-DANN-Images.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1qxACxrnzhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjBNe5q9nzhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ML and numerical libraries\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# DANN paper implementation helpers\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.python.framework import ops\n",
        "import urllib\n",
        "\n",
        "# Text processing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Data visualization\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LChLxiLzoWIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image_from_path(path):\n",
        "    resp = urllib.request.urlopen(os.path.join(\"https://raw.githubusercontent.com/radusqrt/research/master/datasets/crisis-mmd\", path))\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "  \n",
        "\n",
        "def preprocess_image(paths):\n",
        "    paths = [cv2.resize(read_image_from_path(path), (160, 160), interpolation=cv2.INTER_LINEAR) for path in paths]\n",
        "    return paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crrK2GiyokWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlipGradientBuilder(object):\n",
        "    def __init__(self):\n",
        "        self.num_calls = 0\n",
        "\n",
        "    def __call__(self, x, l=1.0):\n",
        "        grad_name = \"FlipGradient%d\" % self.num_calls\n",
        "        @ops.RegisterGradient(grad_name)\n",
        "        def _flip_gradients(op, grad):\n",
        "            return [tf.negative(grad) * l]\n",
        "        \n",
        "        g = tf.compat.v1.get_default_graph()\n",
        "        with g.gradient_override_map({\"Identity\": grad_name}):\n",
        "            y = tf.identity(x)\n",
        "            \n",
        "        self.num_calls += 1\n",
        "        return y\n",
        "    \n",
        "flip_gradient = FlipGradientBuilder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uaiCtWT1zzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(shape):\n",
        "    initial = tf.random.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool2d(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def shuffle_aligned_list(data):\n",
        "    print(data[0].shape)\n",
        "    \"\"\"Shuffle arrays in a list by shuffling each array identically.\"\"\"\n",
        "    num = data[0].shape[0]\n",
        "    p = np.random.permutation(num)\n",
        "    return [d[p] for d in data]\n",
        "\n",
        "\n",
        "# def batch_generator(data, batch_size, shuffle=True):\n",
        "def batch_generator(data, batch_size, shuffle=False):\n",
        "    \"\"\"Generate batches of data.\n",
        "    \n",
        "    Given a list of array-like objects, generate batches of a given\n",
        "    size by yielding a list of array-like objects corresponding to the\n",
        "    same slice of each input.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        data = shuffle_aligned_list(data)\n",
        "\n",
        "    batch_count = 0\n",
        "    while True:\n",
        "        if batch_count * batch_size + batch_size >= len(data[0]):\n",
        "            batch_count = 0\n",
        "\n",
        "            if shuffle:\n",
        "                data = shuffle_aligned_list(data)\n",
        "\n",
        "        start = batch_count * batch_size\n",
        "        end = start + batch_size\n",
        "        batch_count += 1\n",
        "        yield [d[start:end] for d in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrQIm0gjnzh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mandatory for CUDA, NVIDIA, Linux Mint compatibility with middle-level TF code\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = InteractiveSession(config=config)\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACS98KGAnziB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useless columns\n",
        "useless_columns = ['tweet_id', 'image_id', 'text_info', 'text_info_conf', 'image_info_conf', 'text_human', 'text_human_conf', 'image_human', 'image_human_conf', 'tweet_text', 'image_url']\n",
        "\n",
        "# Read and preprocess datasets\n",
        "hurricane_harvey = pd.read_csv(\n",
        "        'https://raw.githubusercontent.com/radusqrt/research/master/datasets/crisis-mmd/annotations/hurricane_harvey_final_data.tsv', sep='\\t', header=0).drop(useless_columns, axis=1)\n",
        "hurricane_irma = pd.read_csv(\n",
        "        'https://github.com/radusqrt/research/blob/master/datasets/crisis-mmd/annotations/hurricane_irma_final_data.tsv?raw=true', sep='\\t', header=0).drop(useless_columns, axis=1)\n",
        "hurricane_maria = pd.read_csv(\n",
        "        'https://github.com/radusqrt/research/blob/master/datasets/crisis-mmd/annotations/hurricane_maria_final_data.tsv?raw=true', sep='\\t', header=0).drop(useless_columns, axis=1)\n",
        "source_df = pd.concat([hurricane_harvey, hurricane_irma, hurricane_maria])\n",
        "source_df = source_df[source_df.image_damage.notnull()]\n",
        "source_df = pd.concat([source_df, pd.get_dummies(source_df['image_damage'])], axis=1).drop(['image_damage'], axis=1)\n",
        "\n",
        "target_df = pd.read_csv(\n",
        "        'https://raw.githubusercontent.com/radusqrt/research/master/datasets/crisis-mmd/annotations/california_wildfires_final_data.tsv', sep='\\t', header=0).drop(useless_columns, axis=1)\n",
        "target_df = target_df[target_df.image_damage.notnull()]\n",
        "target_df = pd.concat([target_df, pd.get_dummies(target_df['image_damage'])], axis=1).drop(['image_damage'], axis=1)\n",
        "\n",
        "source_examples = preprocess_image(source_df['image_path'])\n",
        "source_labels = source_df[['severe_damage', 'little_or_no_damage', 'mild_damage']]\n",
        "\n",
        "target_examples = preprocess_image(target_df['image_path'])\n",
        "target_labels = target_df[['severe_damage', 'little_or_no_damage', 'mild_damage']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MduzJAcRnziK",
        "colab_type": "code",
        "outputId": "ea7edd75-64ea-4d81-a78a-e1fb674a6155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = np.array(source_examples)\n",
        "X_target = np.array(target_examples)\n",
        "source_labels = np.array(source_labels)\n",
        "target_labels = np.array(target_labels)\n",
        "\n",
        "# Split training and testing data\n",
        "X_source_train, X_source_test, y_source_train, y_source_test = \\\n",
        "    train_test_split(X, source_labels, test_size=0.3, random_state=42)\n",
        "X_target_train, X_target_test, y_target_train, y_target_test = \\\n",
        "    train_test_split(X_target, target_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "print(X_source_train.shape, X_source_test.shape, X_target_train.shape, X_target_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1847, 160, 160, 3) (792, 160, 160, 3) (381, 160, 160, 3) (164, 160, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ctQxhxnziV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mixed dataset for TSNE visualization\n",
        "num_test = 150\n",
        "combined_test_imgs = np.vstack([X_source_test[:num_test], X_target_test[:num_test]])\n",
        "combined_test_labels = np.vstack([y_source_test[:num_test], y_target_test[:num_test]])\n",
        "combined_test_domain = np.vstack([np.tile([1., 0.], [num_test, 1]),\n",
        "        np.tile([0., 1.], [num_test, 1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3d8dLFInzid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "class DANN_Model(object):\n",
        "    \"\"\"Domain adaptation model.\"\"\"\n",
        "    def __init__(self):\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.X = tf.placeholder(tf.uint8, [None, 160, 160, 3])\n",
        "        self.y = tf.placeholder(tf.float32, [None, 3])\n",
        "        self.domain = tf.placeholder(tf.float32, [None, 2])\n",
        "        self.l = tf.placeholder(tf.float32, [])\n",
        "        self.train = tf.placeholder(tf.bool, [])\n",
        "        \n",
        "        # CNN model for feature extraction\n",
        "        with tf.variable_scope('feature_extractor'):\n",
        "\n",
        "            W_conv0 = weight_variable([5, 5, 3, 32])\n",
        "            b_conv0 = bias_variable([32])\n",
        "            h_conv0 = tf.nn.relu(conv2d(tf.cast(self.X, tf.float32), W_conv0) + b_conv0)\n",
        "            h_pool0 = max_pool_2x2(h_conv0)\n",
        "            \n",
        "            W_conv1 = weight_variable([5, 5, 32, 48])\n",
        "            b_conv1 = bias_variable([48])\n",
        "            h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1)\n",
        "            h_pool1 = max_pool_2x2(h_conv1)\n",
        "            \n",
        "            # The domain-invariant feature\n",
        "            self.feature = tf.reshape(h_pool1, [-1, 40*40*48])\n",
        "\n",
        "        # MLP for class prediction\n",
        "        with tf.variable_scope('label_predictor'):\n",
        "            \n",
        "            # Switches to route target examples (second half of batch) differently\n",
        "            # depending on train or test mode.\n",
        "            all_features = lambda: self.feature\n",
        "            source_features = lambda: tf.slice(self.feature, [0, 0], [batch_size // 2, -1])\n",
        "            classify_feats = tf.cond(self.train, source_features, all_features)\n",
        "            \n",
        "            all_labels = lambda: self.y\n",
        "            source_labels = lambda: tf.slice(self.y, [0, 0], [batch_size // 2, -1])\n",
        "            self.classify_labels = tf.cond(self.train, source_labels, all_labels)\n",
        "            \n",
        "            W_fc0 = weight_variable([40 * 40 * 48, 100])\n",
        "            b_fc0 = bias_variable([100])\n",
        "            h_fc0 = tf.nn.relu(tf.matmul(classify_feats, W_fc0) + b_fc0)\n",
        "\n",
        "            W_fc1 = weight_variable([100, 100])\n",
        "            b_fc1 = bias_variable([100])\n",
        "            h_fc1 = tf.nn.relu(tf.matmul(h_fc0, W_fc1) + b_fc1)\n",
        "\n",
        "            W_fc2 = weight_variable([100, 3])\n",
        "            b_fc2 = bias_variable([3])\n",
        "            logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "            \n",
        "            self.pred = tf.nn.softmax(logits)\n",
        "            self.pred_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=self.classify_labels)\n",
        "\n",
        "        # Small MLP for domain prediction with adversarial loss\n",
        "        with tf.variable_scope('domain_predictor'):\n",
        "            \n",
        "            # Flip the gradient when backpropagating through this operation\n",
        "            feat = flip_gradient(self.feature, self.l)\n",
        "            \n",
        "            d_W_fc0 = weight_variable([40 * 40 * 48, 100])\n",
        "            d_b_fc0 = bias_variable([100])\n",
        "            d_h_fc0 = tf.nn.relu(tf.matmul(feat, d_W_fc0) + d_b_fc0)\n",
        "            \n",
        "            d_W_fc1 = weight_variable([100, 2])\n",
        "            d_b_fc1 = bias_variable([2])\n",
        "            d_logits = tf.matmul(d_h_fc0, d_W_fc1) + d_b_fc1\n",
        "            \n",
        "            self.domain_pred = tf.nn.softmax(d_logits)\n",
        "            self.domain_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=d_logits, labels=self.domain)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO2nQcUAnzik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model graph\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "with graph.as_default():\n",
        "    model = DANN_Model()\n",
        "    \n",
        "    # Training\n",
        "    learning_rate = tf.placeholder(tf.float32, [])\n",
        "    \n",
        "    pred_loss = tf.reduce_mean(model.pred_loss)\n",
        "    domain_loss = tf.reduce_mean(model.domain_loss)\n",
        "    total_loss = pred_loss + domain_loss\n",
        "\n",
        "    regular_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(pred_loss)\n",
        "    dann_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(total_loss)\n",
        "    \n",
        "    # Evaluation\n",
        "    correct_label_pred = tf.equal(tf.argmax(model.classify_labels, 1), tf.argmax(model.pred, 1))\n",
        "    label_acc = tf.reduce_mean(tf.cast(correct_label_pred, tf.float32))\n",
        "    correct_domain_pred = tf.equal(tf.argmax(model.domain, 1), tf.argmax(model.domain_pred, 1))\n",
        "    domain_acc = tf.reduce_mean(tf.cast(correct_domain_pred, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SKr2VzXUnzis",
        "colab_type": "code",
        "outputId": "8ced6fe7-9d35-434c-f371-2df17db723eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "def train_and_evaluate(training_mode, graph, model, num_steps=2000, verbose=True):\n",
        "    \"\"\"Helper to run the model with different training modes.\"\"\"\n",
        "\n",
        "#     with session as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    # Batch generators\n",
        "    gen_source_batch = batch_generator(\n",
        "        [X_source_train, y_source_train], batch_size // 2)\n",
        "    gen_target_batch = batch_generator(\n",
        "        [X_target_train, y_target_train], batch_size // 2)\n",
        "    gen_source_only_batch = batch_generator(\n",
        "        [X_source_train, y_source_train], batch_size)\n",
        "    gen_target_only_batch = batch_generator(\n",
        "        [X_target_train, y_target_train], batch_size)\n",
        "\n",
        "    domain_labels = np.vstack([np.tile([1., 0.], [batch_size // 2, 1]),\n",
        "                               np.tile([0., 1.], [batch_size // 2, 1])])\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_steps):\n",
        "        if i % 500 == 0:\n",
        "            print('step {0} out of {1}'.format(i, num_steps))\n",
        "        # Adaptation param and learning rate schedule as described in the paper\n",
        "        p = float(i) / num_steps\n",
        "        l = 2. / (1. + np.exp(-10. * p)) - 1\n",
        "        lr = 0.0001 / (1. + 10 * p)**0.75\n",
        "\n",
        "        # Training step\n",
        "        if training_mode == 'dann':\n",
        "\n",
        "            X0, y0 = next(gen_source_batch)\n",
        "            X1, y1 = next(gen_target_batch)\n",
        "            X = np.vstack([X0, X1])\n",
        "            y = np.vstack([y0, y1])\n",
        "\n",
        "            _, batch_loss, dloss, ploss, d_acc, p_acc = sess.run(\n",
        "                [dann_train_op, total_loss, domain_loss, pred_loss, domain_acc, label_acc],\n",
        "                feed_dict={model.X: X, model.y: y, model.domain: domain_labels,\n",
        "                           model.train: True, model.l: l, learning_rate: lr})\n",
        "\n",
        "            if verbose and i % 100 == 0:\n",
        "                print('loss: {}  d_acc: {}  p_acc: {}  p: {}  l: {}  lr: {}'.format(\n",
        "                        batch_loss, d_acc, p_acc, p, l, lr))\n",
        "\n",
        "        elif training_mode == 'source':\n",
        "            X, y = next(gen_source_only_batch)\n",
        "            _, batch_loss = sess.run([regular_train_op, pred_loss],\n",
        "                                 feed_dict={model.X: X, model.y: y, model.train: False,\n",
        "                                            model.l: l, learning_rate: lr})\n",
        "            \n",
        "            if verbose and i % 100 == 0:\n",
        "                print('loss: {}  p: {}  l: {}  lr: {}'.format(\n",
        "                        batch_loss, p, l, lr))\n",
        "\n",
        "        elif training_mode == 'target':\n",
        "            X, y = next(gen_target_only_batch)\n",
        "            _, batch_loss = sess.run([regular_train_op, pred_loss],\n",
        "                                 feed_dict={model.X: X, model.y: y, model.train: False,\n",
        "                                            model.l: l, learning_rate: lr})\n",
        "\n",
        "    # Compute final evaluation on test data\n",
        "    source_acc = sess.run(label_acc,\n",
        "                        feed_dict={model.X: X_source_test, model.y: y_source_test,\n",
        "                                   model.train: False})\n",
        "\n",
        "    target_acc = sess.run(label_acc,\n",
        "                        feed_dict={model.X: X_target_test, model.y: y_target_test,\n",
        "                                   model.train: False})\n",
        "\n",
        "    test_domain_acc = sess.run(domain_acc,\n",
        "                        feed_dict={model.X: combined_test_imgs,\n",
        "                                   model.domain: combined_test_domain, model.l: 1.0})\n",
        "\n",
        "    test_emb = sess.run(model.feature, feed_dict={model.X: combined_test_imgs})\n",
        "    return source_acc, target_acc, test_domain_acc, test_emb\n",
        "\n",
        "\n",
        "print('\\nSource only training')\n",
        "source_acc, target_acc, _, source_only_emb = train_and_evaluate('source', graph, model)\n",
        "print('Source accuracy:', source_acc)\n",
        "print('Target accuracy:', target_acc)\n",
        "\n",
        "print('\\nDomain adaptation training')\n",
        "source_acc, target_acc, d_acc, dann_emb = train_and_evaluate('dann', graph, model)\n",
        "print('Source accuracy:', source_acc)\n",
        "print('Target accuracy:', target_acc)\n",
        "print('Domain accuracy:', d_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Source only training\n",
            "step 0 out of 2000\n",
            "loss: 1290.6116943359375  p: 0.0  l: 0.0  lr: 0.0001\n",
            "loss: 0.9948427677154541  p: 0.05  l: 0.2449186624037092  lr: 7.377879464668812e-05\n",
            "loss: 0.9356509447097778  p: 0.1  l: 0.4621171572600098  lr: 5.946035575013606e-05\n",
            "loss: 1.0079995393753052  p: 0.15  l: 0.6351489523872873  lr: 5.029733718731742e-05\n",
            "loss: 0.9941279888153076  p: 0.2  l: 0.7615941559557646  lr: 4.3869133765083086e-05\n",
            "step 500 out of 2000\n",
            "loss: 0.9755426645278931  p: 0.25  l: 0.8482836399575131  lr: 3.907949713906801e-05\n",
            "loss: 0.9790993928909302  p: 0.3  l: 0.9051482536448667  lr: 3.535533905932738e-05\n",
            "loss: 0.9543672800064087  p: 0.35  l: 0.9413755384972873  lr: 3.236611811382156e-05\n",
            "loss: 0.9642907977104187  p: 0.4  l: 0.9640275800758169  lr: 2.9906975624424413e-05\n",
            "loss: 0.9394042491912842  p: 0.45  l: 0.9780261147388136  lr: 2.78437664873526e-05\n",
            "step 1000 out of 2000\n",
            "loss: 1.015130639076233  p: 0.5  l: 0.9866142981514305  lr: 2.6084743001221458e-05\n",
            "loss: 0.9936988949775696  p: 0.55  l: 0.9918597245682079  lr: 2.4564898981344157e-05\n",
            "loss: 0.9704803824424744  p: 0.6  l: 0.9950547536867307  lr: 2.3236808024254083e-05\n",
            "loss: 0.9734463691711426  p: 0.65  l: 0.996997635486526  lr: 2.2065006130979154e-05\n",
            "loss: 0.9550409913063049  p: 0.7  l: 0.9981778976111988  lr: 2.1022410381342864e-05\n",
            "step 1500 out of 2000\n",
            "loss: 0.9560834169387817  p: 0.75  l: 0.9988944427261528  lr: 2.008795864910758e-05\n",
            "loss: 0.9457418918609619  p: 0.8  l: 0.9993292997390673  lr: 1.9245008972987527e-05\n",
            "loss: 1.0089964866638184  p: 0.85  l: 0.9995931460438896  lr: 1.848022676613902e-05\n",
            "loss: 0.9938029646873474  p: 0.9  l: 0.9997532108480274  lr: 1.778279410038923e-05\n",
            "loss: 0.9660056233406067  p: 0.95  l: 0.999850307544979  lr: 1.7143836874659566e-05\n",
            "Source accuracy: 0.5542929\n",
            "Target accuracy: 0.86585367\n",
            "\n",
            "Domain adaptation training\n",
            "step 0 out of 2000\n",
            "loss: 2329.37744140625  d_acc: 0.5  p_acc: 0.21875  p: 0.0  l: 0.0  lr: 0.0001\n",
            "loss: 1.8073451519012451  d_acc: 0.5  p_acc: 0.5  p: 0.05  l: 0.2449186624037092  lr: 7.377879464668812e-05\n",
            "loss: 1.5987542867660522  d_acc: 0.5  p_acc: 0.53125  p: 0.1  l: 0.4621171572600098  lr: 5.946035575013606e-05\n",
            "loss: 1.7724871635437012  d_acc: 0.5  p_acc: 0.46875  p: 0.15  l: 0.6351489523872873  lr: 5.029733718731742e-05\n",
            "loss: 1.6688523292541504  d_acc: 0.5  p_acc: 0.5  p: 0.2  l: 0.7615941559557646  lr: 4.3869133765083086e-05\n",
            "step 500 out of 2000\n",
            "loss: 1.7568471431732178  d_acc: 0.5  p_acc: 0.53125  p: 0.25  l: 0.8482836399575131  lr: 3.907949713906801e-05\n",
            "loss: 1.675141453742981  d_acc: 0.5  p_acc: 0.5625  p: 0.3  l: 0.9051482536448667  lr: 3.535533905932738e-05\n",
            "loss: 1.6570188999176025  d_acc: 0.5  p_acc: 0.53125  p: 0.35  l: 0.9413755384972873  lr: 3.236611811382156e-05\n",
            "loss: 1.782326579093933  d_acc: 0.5  p_acc: 0.46875  p: 0.4  l: 0.9640275800758169  lr: 2.9906975624424413e-05\n",
            "loss: 1.5793918371200562  d_acc: 0.5  p_acc: 0.65625  p: 0.45  l: 0.9780261147388136  lr: 2.78437664873526e-05\n",
            "step 1000 out of 2000\n",
            "loss: 1.6324470043182373  d_acc: 0.5  p_acc: 0.5625  p: 0.5  l: 0.9866142981514305  lr: 2.6084743001221458e-05\n",
            "loss: 1.7141313552856445  d_acc: 0.5  p_acc: 0.53125  p: 0.55  l: 0.9918597245682079  lr: 2.4564898981344157e-05\n",
            "loss: 1.6396970748901367  d_acc: 0.5  p_acc: 0.59375  p: 0.6  l: 0.9950547536867307  lr: 2.3236808024254083e-05\n",
            "loss: 1.638094186782837  d_acc: 0.5  p_acc: 0.5625  p: 0.65  l: 0.996997635486526  lr: 2.2065006130979154e-05\n",
            "loss: 1.6325135231018066  d_acc: 0.5  p_acc: 0.5625  p: 0.7  l: 0.9981778976111988  lr: 2.1022410381342864e-05\n",
            "step 1500 out of 2000\n",
            "loss: 1.6466648578643799  d_acc: 0.5  p_acc: 0.53125  p: 0.75  l: 0.9988944427261528  lr: 2.008795864910758e-05\n",
            "loss: 1.5998340845108032  d_acc: 0.5  p_acc: 0.5625  p: 0.8  l: 0.9993292997390673  lr: 1.9245008972987527e-05\n",
            "loss: 1.7000197172164917  d_acc: 0.5  p_acc: 0.53125  p: 0.85  l: 0.9995931460438896  lr: 1.848022676613902e-05\n",
            "loss: 1.6537796258926392  d_acc: 0.5  p_acc: 0.625  p: 0.9  l: 0.9997532108480274  lr: 1.778279410038923e-05\n",
            "loss: 1.6322600841522217  d_acc: 0.5  p_acc: 0.59375  p: 0.95  l: 0.999850307544979  lr: 1.7143836874659566e-05\n",
            "Source accuracy: 0.5530303\n",
            "Target accuracy: 0.8719512\n",
            "Domain accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwvdWdh9nzix",
        "colab_type": "code",
        "outputId": "da75f549-e628-4922-c21c-13ec8de84c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
        "source_only_tsne = tsne.fit_transform(source_only_emb)\n",
        "\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
        "dann_tsne = tsne.fit_transform(dann_emb)\n",
        "        \n",
        "plot_embedding(source_only_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Source only')\n",
        "plot_embedding(dann_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Domain Adaptation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/pca.py:536: RuntimeWarning: invalid value encountered in true_divide\n",
            "  self.explained_variance_ / total_var.sum()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-37ceeb6bc2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdann_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdann_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplot_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_only_tsne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_test_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_test_domain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Source only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplot_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdann_tsne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_test_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_test_domain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Domain Adaptation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_embedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz0HXRSXnzi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha2YSDDwnzi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}