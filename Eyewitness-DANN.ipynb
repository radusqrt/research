{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML and numerical libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DANN paper implementation helpers\n",
    "from flip_gradient import flip_gradient\n",
    "from utils import *\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Data visualization\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandatory for CUDA, NVIDIA, Linux Mint compatibility with middle-level TF code\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = InteractiveSession(config=config)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess datasets\n",
    "source_df = one_hot_eyewitness(\n",
    "    pd.read_csv(\n",
    "        'datasets/datasets-eyewitness/EyewitnessTweetsFromCrisisT26/CrisisLexT26_FloodsTraining_EN.csv'))\n",
    "target_df = one_hot_eyewitness(\n",
    "    pd.read_csv(\n",
    "        'datasets/datasets-eyewitness/EyewitnessTweetsFromCrisisT26/CrisisLexT26_FloodsTest_EN.csv'))\n",
    "\n",
    "source_examples = preprocess_text(source_df['sentence'])\n",
    "source_labels = source_df[['Eyewitness', 'NotEyewitness']]\n",
    "\n",
    "target_examples = preprocess_text(target_df['sentence'])\n",
    "target_labels = target_df[['Eyewitness', 'NotEyewitness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_features size as the input convolutional matrix size\n",
    "cv = CountVectorizer(max_features=35*35*5)\n",
    "\n",
    "# Fit on the training examples and transform the datasets\n",
    "cv.fit(source_examples)\n",
    "X = cv.transform(source_examples).todense()\n",
    "X_target = cv.transform(target_examples).todense()\n",
    "\n",
    "# Change shape to convolutional matrix\n",
    "X = np.array(X).reshape((X.shape[0], 35, 35, 5))\n",
    "X_target = np.array(X_target).reshape((X_target.shape[0], 35, 35, 5))\n",
    "\n",
    "# Split training and testing data\n",
    "X_source_train, X_source_test, y_source_train, y_source_test = \\\n",
    "    train_test_split(X, source_labels, test_size=0.3, random_state=None)\n",
    "X_target_train, X_target_test, y_target_train, y_target_test = \\\n",
    "    train_test_split(X_target, target_labels, test_size=0.3, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mixed dataset for TSNE visualization\n",
    "num_test = 150\n",
    "combined_test_imgs = np.vstack([X_source_test[:num_test], X_target_test[:num_test]])\n",
    "combined_test_labels = np.vstack([y_source_test[:num_test], y_target_test[:num_test]])\n",
    "combined_test_domain = np.vstack([np.tile([1., 0.], [num_test, 1]),\n",
    "        np.tile([0., 1.], [num_test, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "class DANN_Model(object):\n",
    "    \"\"\"Domain adaptation model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.X = tf.placeholder(tf.uint8, [None, 35, 35, 5])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 2])\n",
    "        self.domain = tf.placeholder(tf.float32, [None, 2])\n",
    "        self.l = tf.placeholder(tf.float32, [])\n",
    "        self.train = tf.placeholder(tf.bool, [])\n",
    "        \n",
    "        # CNN model for feature extraction\n",
    "        with tf.variable_scope('feature_extractor'):\n",
    "\n",
    "            W_conv0 = weight_variable([5, 5, 5, 32])\n",
    "            b_conv0 = bias_variable([32])\n",
    "            h_conv0 = tf.nn.relu(conv2d(tf.cast(self.X, tf.float32), W_conv0) + b_conv0)\n",
    "            h_pool0 = max_pool_2x2(h_conv0)\n",
    "            \n",
    "            W_conv1 = weight_variable([5, 5, 32, 48])\n",
    "            b_conv1 = bias_variable([48])\n",
    "            h_conv1 = tf.nn.relu(conv2d(h_pool0, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            \n",
    "            # The domain-invariant feature\n",
    "            self.feature = tf.reshape(h_pool1, [-1, 9*9*48])\n",
    "\n",
    "        # MLP for class prediction\n",
    "        with tf.variable_scope('label_predictor'):\n",
    "            \n",
    "            # Switches to route target examples (second half of batch) differently\n",
    "            # depending on train or test mode.\n",
    "            all_features = lambda: self.feature\n",
    "            source_features = lambda: tf.slice(self.feature, [0, 0], [batch_size // 2, -1])\n",
    "            classify_feats = tf.cond(self.train, source_features, all_features)\n",
    "            \n",
    "            all_labels = lambda: self.y\n",
    "            source_labels = lambda: tf.slice(self.y, [0, 0], [batch_size // 2, -1])\n",
    "            self.classify_labels = tf.cond(self.train, source_labels, all_labels)\n",
    "            \n",
    "            W_fc0 = weight_variable([9 * 9 * 48, 100])\n",
    "            b_fc0 = bias_variable([100])\n",
    "            h_fc0 = tf.nn.relu(tf.matmul(classify_feats, W_fc0) + b_fc0)\n",
    "\n",
    "            W_fc1 = weight_variable([100, 100])\n",
    "            b_fc1 = bias_variable([100])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_fc0, W_fc1) + b_fc1)\n",
    "\n",
    "            W_fc2 = weight_variable([100, 2])\n",
    "            b_fc2 = bias_variable([2])\n",
    "            logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "            \n",
    "            self.pred = tf.nn.softmax(logits)\n",
    "            self.pred_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=self.classify_labels)\n",
    "\n",
    "        # Small MLP for domain prediction with adversarial loss\n",
    "        with tf.variable_scope('domain_predictor'):\n",
    "            \n",
    "            # Flip the gradient when backpropagating through this operation\n",
    "            feat = flip_gradient(self.feature, self.l)\n",
    "            \n",
    "            d_W_fc0 = weight_variable([9 * 9 * 48, 100])\n",
    "            d_b_fc0 = bias_variable([100])\n",
    "            d_h_fc0 = tf.nn.relu(tf.matmul(feat, d_W_fc0) + d_b_fc0)\n",
    "            \n",
    "            d_W_fc1 = weight_variable([100, 2])\n",
    "            d_b_fc1 = bias_variable([2])\n",
    "            d_logits = tf.matmul(d_h_fc0, d_W_fc1) + d_b_fc1\n",
    "            \n",
    "            self.domain_pred = tf.nn.softmax(d_logits)\n",
    "            self.domain_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=d_logits, labels=self.domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model graph\n",
    "graph = tf.compat.v1.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model = DANN_Model()\n",
    "    \n",
    "    # Training\n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    \n",
    "    pred_loss = tf.reduce_mean(model.pred_loss)\n",
    "    domain_loss = tf.reduce_mean(model.domain_loss)\n",
    "    total_loss = pred_loss + domain_loss\n",
    "\n",
    "    regular_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(pred_loss)\n",
    "    dann_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(total_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    correct_label_pred = tf.equal(tf.argmax(model.classify_labels, 1), tf.argmax(model.pred, 1))\n",
    "    label_acc = tf.reduce_mean(tf.cast(correct_label_pred, tf.float32))\n",
    "    correct_domain_pred = tf.equal(tf.argmax(model.domain, 1), tf.argmax(model.domain_pred, 1))\n",
    "    domain_acc = tf.reduce_mean(tf.cast(correct_domain_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source only training\n",
      "step 0 out of 8600\n",
      "step 500 out of 8600\n",
      "step 1000 out of 8600\n",
      "step 1500 out of 8600\n",
      "step 2000 out of 8600\n",
      "step 2500 out of 8600\n",
      "step 3000 out of 8600\n",
      "step 3500 out of 8600\n",
      "step 4000 out of 8600\n",
      "step 4500 out of 8600\n",
      "step 5000 out of 8600\n",
      "step 5500 out of 8600\n",
      "step 6000 out of 8600\n",
      "step 6500 out of 8600\n",
      "step 7000 out of 8600\n",
      "step 7500 out of 8600\n",
      "step 8000 out of 8600\n",
      "step 8500 out of 8600\n",
      "Source accuracy: 0.8097561\n",
      "Target accuracy: 0.855\n",
      "\n",
      "Domain adaptation training\n",
      "step 0 out of 8600\n",
      "loss: 1.714733362197876  d_acc: 0.5  p_acc: 0.3125  p: 0.0  l: 0.0  lr: 0.01\n",
      "loss: 1.0547065734863281  d_acc: 0.734375  p_acc: 0.765625  p: 0.011627906976744186  l: 0.05807411547586794  lr: 0.009208108196143174\n",
      "loss: 1.0706374645233154  d_acc: 0.7578125  p_acc: 0.71875  p: 0.023255813953488372  l: 0.11575782577418603  lr: 0.008548589038554535\n",
      "loss: 1.1653077602386475  d_acc: 0.75  p_acc: 0.75  p: 0.03488372093023256  l: 0.1726711536624863  lr: 0.007989697680708598\n",
      "loss: 0.750344455242157  d_acc: 0.7578125  p_acc: 0.890625  p: 0.046511627906976744  l: 0.22845439143629886  lr: 0.0075092390442056635\n",
      "step 500 out of 8600\n",
      "loss: 0.6528230905532837  d_acc: 0.765625  p_acc: 0.921875  p: 0.05813953488372093  l: 0.28277682569099527  lr: 0.0070911987391239035\n",
      "loss: 1.8496843576431274  d_acc: 0.5  p_acc: 0.9375  p: 0.06976744186046512  l: 0.33534391863054624  lr: 0.006723713451795381\n",
      "loss: 0.6580154895782471  d_acc: 0.6953125  p_acc: 0.9375  p: 0.08139534883720931  l: 0.3859026564904293  lr: 0.006397796023621206\n",
      "loss: 0.45841649174690247  d_acc: 0.828125  p_acc: 1.0  p: 0.09302325581395349  l: 0.43424492823161986  lr: 0.006106505778508959\n",
      "loss: 1.0011334419250488  d_acc: 0.5234375  p_acc: 0.921875  p: 0.10465116279069768  l: 0.4802089471455322  lr: 0.00584439199954653\n",
      "step 1000 out of 8600\n",
      "loss: 1.734921932220459  d_acc: 0.5625  p_acc: 0.828125  p: 0.11627906976744186  l: 0.5236788585597902  lr: 0.0056071106987353935\n",
      "loss: 0.878650426864624  d_acc: 0.625  p_acc: 0.90625  p: 0.12790697674418605  l: 0.5645827773148586  lr: 0.005391154585323163\n",
      "loss: 1.2634689807891846  d_acc: 0.6015625  p_acc: 0.796875  p: 0.13953488372093023  l: 0.6028895635619975  lr: 0.005193658897076379\n",
      "loss: 1.226973056793213  d_acc: 0.703125  p_acc: 0.734375  p: 0.1511627906976744  l: 0.6386046745549958  lr: 0.005012259239308981\n",
      "loss: 1.500573754310608  d_acc: 0.5  p_acc: 0.734375  p: 0.16279069767441862  l: 0.6717654275930591  lr: 0.004844985806046933\n",
      "step 1500 out of 8600\n",
      "loss: 1.1892743110656738  d_acc: 0.5625  p_acc: 0.796875  p: 0.1744186046511628  l: 0.7024359819836457  lr: 0.004690183518397721\n",
      "loss: 1.1942996978759766  d_acc: 0.5  p_acc: 0.828125  p: 0.18604651162790697  l: 0.730702303851418  lr: 0.0045464509301225125\n",
      "loss: 1.239026665687561  d_acc: 0.5625  p_acc: 0.75  p: 0.19767441860465115  l: 0.75666732465968  lr: 0.004412592926309448\n",
      "loss: 1.3571579456329346  d_acc: 0.5  p_acc: 0.65625  p: 0.20930232558139536  l: 0.7804464491564882  lr: 0.004287583697558238\n",
      "loss: 1.3425565958023071  d_acc: 0.5078125  p_acc: 0.671875  p: 0.22093023255813954  l: 0.8021635162227865  lr: 0.004170537464590465\n",
      "step 2000 out of 8600\n",
      "loss: 1.238600254058838  d_acc: 0.5  p_acc: 0.765625  p: 0.23255813953488372  l: 0.8219472701703336  lr: 0.00406068511562926\n",
      "loss: 1.2391927242279053  d_acc: 0.46875  p_acc: 0.765625  p: 0.2441860465116279  l: 0.8399283622203215  lr: 0.003957355402193208\n",
      "loss: 1.2346197366714478  d_acc: 0.5625  p_acc: 0.765625  p: 0.2558139534883721  l: 0.8562368727213061  lr: 0.003859959683449113\n",
      "loss: 1.2400219440460205  d_acc: 0.5  p_acc: 0.765625  p: 0.26744186046511625  l: 0.8710003237511923  lr: 0.0037679794579745665\n",
      "loss: 1.119713544845581  d_acc: 0.5  p_acc: 0.875  p: 0.27906976744186046  l: 0.8843421381310939  lr: 0.0036809561034616242\n",
      "step 2500 out of 8600\n",
      "loss: 1.1844631433486938  d_acc: 0.46875  p_acc: 0.8125  p: 0.29069767441860467  l: 0.8963804933094455  lr: 0.0035984823790740587\n",
      "loss: 1.3262540102005005  d_acc: 0.5078125  p_acc: 0.6875  p: 0.3023255813953488  l: 0.907227515730854  lr: 0.0035201953452889123\n",
      "loss: 1.3258278369903564  d_acc: 0.5  p_acc: 0.6875  p: 0.313953488372093  l: 0.9169887619362538  lr: 0.0034457704314723612\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(training_mode, graph, model, num_steps=8600, verbose=True):\n",
    "    \"\"\"Helper to run the model with different training modes.\"\"\"\n",
    "\n",
    "#     with session as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Batch generators\n",
    "    gen_source_batch = batch_generator(\n",
    "        [X_source_train, y_source_train], batch_size // 2)\n",
    "    gen_target_batch = batch_generator(\n",
    "        [X_target_train, y_target_train], batch_size // 2)\n",
    "    gen_source_only_batch = batch_generator(\n",
    "        [X_source_train, y_source_train], batch_size)\n",
    "    gen_target_only_batch = batch_generator(\n",
    "        [X_target_train, y_target_train], batch_size)\n",
    "\n",
    "    domain_labels = np.vstack([np.tile([1., 0.], [batch_size // 2, 1]),\n",
    "                               np.tile([0., 1.], [batch_size // 2, 1])])\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(num_steps):\n",
    "        if i % 500 == 0:\n",
    "            print('step {0} out of {1}'.format(i, num_steps))\n",
    "        # Adaptation param and learning rate schedule as described in the paper\n",
    "        p = float(i) / num_steps\n",
    "        l = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "        lr = 0.01 / (1. + 10 * p)**0.75\n",
    "\n",
    "        # Training step\n",
    "        if training_mode == 'dann':\n",
    "\n",
    "            X0, y0 = next(gen_source_batch)\n",
    "            X1, y1 = next(gen_target_batch)\n",
    "            X = np.vstack([X0, X1])\n",
    "            y = np.vstack([y0, y1])\n",
    "\n",
    "            _, batch_loss, dloss, ploss, d_acc, p_acc = sess.run(\n",
    "                [dann_train_op, total_loss, domain_loss, pred_loss, domain_acc, label_acc],\n",
    "                feed_dict={model.X: X, model.y: y, model.domain: domain_labels,\n",
    "                           model.train: True, model.l: l, learning_rate: lr})\n",
    "\n",
    "            if verbose and i % 100 == 0:\n",
    "                print('loss: {}  d_acc: {}  p_acc: {}  p: {}  l: {}  lr: {}'.format(\n",
    "                        batch_loss, d_acc, p_acc, p, l, lr))\n",
    "\n",
    "        elif training_mode == 'source':\n",
    "            X, y = next(gen_source_only_batch)\n",
    "            _, batch_loss = sess.run([regular_train_op, pred_loss],\n",
    "                                 feed_dict={model.X: X, model.y: y, model.train: False,\n",
    "                                            model.l: l, learning_rate: lr})\n",
    "\n",
    "        elif training_mode == 'target':\n",
    "            X, y = next(gen_target_only_batch)\n",
    "            _, batch_loss = sess.run([regular_train_op, pred_loss],\n",
    "                                 feed_dict={model.X: X, model.y: y, model.train: False,\n",
    "                                            model.l: l, learning_rate: lr})\n",
    "\n",
    "    # Compute final evaluation on test data\n",
    "    source_acc = sess.run(label_acc,\n",
    "                        feed_dict={model.X: X_source_test, model.y: y_source_test,\n",
    "                                   model.train: False})\n",
    "\n",
    "    target_acc = sess.run(label_acc,\n",
    "                        feed_dict={model.X: X_target_test, model.y: y_target_test,\n",
    "                                   model.train: False})\n",
    "\n",
    "    test_domain_acc = sess.run(domain_acc,\n",
    "                        feed_dict={model.X: combined_test_imgs,\n",
    "                                   model.domain: combined_test_domain, model.l: 1.0})\n",
    "\n",
    "    test_emb = sess.run(model.feature, feed_dict={model.X: combined_test_imgs})\n",
    "    return source_acc, target_acc, test_domain_acc, test_emb\n",
    "\n",
    "\n",
    "print('\\nSource only training')\n",
    "source_acc, target_acc, _, source_only_emb = train_and_evaluate('source', graph, model)\n",
    "print('Source accuracy:', source_acc)\n",
    "print('Target accuracy:', target_acc)\n",
    "\n",
    "print('\\nDomain adaptation training')\n",
    "source_acc, target_acc, d_acc, dann_emb = train_and_evaluate('dann', graph, model)\n",
    "print('Source accuracy:', source_acc)\n",
    "print('Target accuracy:', target_acc)\n",
    "print('Domain accuracy:', d_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
    "source_only_tsne = tsne.fit_transform(source_only_emb)\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
    "dann_tsne = tsne.fit_transform(dann_emb)\n",
    "        \n",
    "plot_embedding(source_only_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Source only')\n",
    "plot_embedding(dann_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Domain Adaptation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
